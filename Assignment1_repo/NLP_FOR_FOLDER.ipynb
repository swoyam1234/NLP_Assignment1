{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28dd3904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize  # Import the sentence tokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "353345b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(sentences_list):\n",
    "    cleaned_sentences = []\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "    for sentence in sentences_list:\n",
    "        # Tokenize each sentence into words and check if it contains a URL\n",
    "        words = sentence.split()\n",
    "        if not any(re.search(url_pattern, word) for word in words):\n",
    "            cleaned_sentences.append(sentence)\n",
    "\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b3506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_sentences(sentences_list):\n",
    "\n",
    "    # Check if the sentence contains only numbers, single letters, only punctuation, or specific symbols\n",
    "    def is_valid(sentence):\n",
    "        return not re.match(r'^[0-9a-zA-Z]*[a-zA-Z][0-9a-zA-Z]*$', sentence) \\\n",
    "               and not re.search(r'\\b\\d{1,3}(,\\d{3})*\\b|\\b\\d{4}\\b', sentence) \\\n",
    "               and 'Â£' not in sentence\n",
    "\n",
    "    valid_sentences = [sentence for sentence in sentences_list if is_valid(sentence)]\n",
    "\n",
    "    return valid_sentences\n",
    "# Call the function to remove invalid sentences\n",
    "# clean_sentences = remove_invalid_sentences(sentences_lower)\n",
    "\n",
    "# Print the valid sentences\n",
    "# print(\"Valid Sentences:\")\n",
    "# for i, clean_sentence in enumerate(clean_sentences):\n",
    "#     print(f\" {i+1}: {clean_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "184c1603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_lists(input_list):\n",
    "    \n",
    "    return [sublist for sublist in input_list if sublist]\n",
    "\n",
    "# clean_sentences = remove_empty_lists(clean_sentences)\n",
    "\n",
    "# for i, clean_sentence in enumerate(clean_sentences):\n",
    "#     print(f\" {i+1}: {clean_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6256efdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /home/student/anaconda3/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /home/student/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.32.1)\n",
      "Requirement already satisfied: tqdm in /home/student/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/student/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: numpy in /home/student/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /home/student/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /home/student/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: nltk in /home/student/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /home/student/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /home/student/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (0.15.1)\n",
      "Requirement already satisfied: Pillow in /home/student/anaconda3/lib/python3.11/site-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: filelock in /home/student/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec in /home/student/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.4.0)\n",
      "Requirement already satisfied: requests in /home/student/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/student/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: sympy in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/student/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/student/anaconda3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/student/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/student/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/student/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.3.2)\n",
      "Requirement already satisfied: click in /home/student/anaconda3/lib/python3.11/site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in /home/student/anaconda3/lib/python3.11/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/student/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/student/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/student/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df2dbf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def sentence_embedding(sentences, model_name='paraphrase-MiniLM-L6-v2'):\n",
    "\n",
    "    # Load pre-trained Sentence-BERT model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Encode sentences to get embeddings\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# embeddings_result = sentence_embedding(clean_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f15ef969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a graph using cosine similarity\n",
    "def create_similarity_graph(embeddings):\n",
    "    G = nx.Graph()\n",
    "    num_sentences = len(embeddings)\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    embeddings = [torch.from_numpy(embedding) for embedding in embeddings]\n",
    "\n",
    "    for i in range(num_sentences):\n",
    "        for j in range(i + 1, num_sentences):\n",
    "            # Convert embeddings to PyTorch tensors\n",
    "            emb_i, emb_j = embeddings[i], embeddings[j]\n",
    "\n",
    "            # Calculate cosine similarity between embeddings\n",
    "            similarity_score = cosine_similarity(emb_i.reshape(1, -1), emb_j.reshape(1, -1))[0, 0]\n",
    "\n",
    "            # Add an edge to the graph with similarity score as weight\n",
    "            G.add_edge(i, j, weight=(1-similarity_score))\n",
    "\n",
    "    return G\n",
    "\n",
    "# graph = create_similarity_graph(embeddings_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4d12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Rank sentences by degree centrality\n",
    "def rank_sentences(graph):\n",
    "    sum_of_weights = {}\n",
    "\n",
    "    for node in graph.nodes():\n",
    "        sum_weight = sum(data['weight'] for _, _, data in graph.edges(node, data=True))\n",
    "        sum_of_weights[node] = sum_weight\n",
    "\n",
    "    return sum_of_weights\n",
    "\n",
    "# ranked_sentences = rank_sentences(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abc86d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Keep a certain number of sentences to create the summary\n",
    "def generate_summary(sentences, ranked_sentences, x):\n",
    "    top_indices = list(ranked_sentences.keys())[:x]\n",
    "\n",
    "    # Select sentences from the list based on the top indices\n",
    "    selected_sentences = [{\"index\": index, \"sentence\": sentences[index]} for index in top_indices]\n",
    "    sorted_sentences = sorted(selected_sentences, key=lambda x: x[\"index\"])\n",
    "    summary = [item[\"sentence\"] for item in sorted_sentences]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b36a917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sentences_to_file(sentences, file_path):\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            for sentence in sentences:\n",
    "                file.write(sentence + '\\n')\n",
    "        print(f\"Sentences saved to {file_path} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3dad7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences saved to /home/student/swoyam/NLP Dataset\\/ss/fns92_92.txt successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"/home/student/swoyam/NLP Dataset\\/s\"\n",
    "output_directory = \"/home/student/swoyam/NLP Dataset\\/ss\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # List all files in the specified folder\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "\n",
    "    # Iterate through each file\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            file_lines = file.readlines()\n",
    "            sentences = [line.strip() for line in file_lines]\n",
    "            sentences_lower = remove_urls(sentences)\n",
    "            clean_sentences = remove_invalid_sentences(sentences_lower)\n",
    "            clean_sentences = remove_empty_lists(clean_sentences)\n",
    "            embeddings_result = sentence_embedding(clean_sentences)\n",
    "            graph = create_similarity_graph(embeddings_result)\n",
    "            ranked_sentences = rank_sentences(graph)\n",
    "            sorted_ranked_sentences = dict(sorted(ranked_sentences.items(), key=lambda x: x[1]))\n",
    "            summary = generate_summary(clean_sentences, sorted_ranked_sentences, 380)\n",
    "            \n",
    "            output_file_name = f\"fns{file_name.split('.')[0]}_{file_name}\"\n",
    "\n",
    "            # Construct the full path for the output file\n",
    "            output_file_path = os.path.join(output_directory, output_file_name)\n",
    "            \n",
    "            save_sentences_to_file(summary, output_file_path)\n",
    "            \n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Folder {folder_path} not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e8cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fdee4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d7df8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e8db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34da1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0cae0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe02c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9a8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e462c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
